{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import torch\n",
    "import cv2 \n",
    "import numpy as np\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def postprocessing(shape, bboxes, margin=1/2):\n",
    "\n",
    "    height = shape[0]\n",
    "    width = shape[1]\n",
    "    \n",
    "    if width>height:\n",
    "        pad = (width-height)/2\n",
    "        bboxes[1]=bboxes[1]-pad\n",
    "    else:\n",
    "        pad = (height-width)/2\n",
    "        bboxes[0]=bboxes[0]-pad\n",
    "    \n",
    "    x_min, y_min, bbox_width, bbox_height = bboxes\n",
    "    origin_bbox = np.array([x_min, y_min, bbox_width, bbox_height]).astype(np.int64).tolist()\n",
    "\n",
    "    #return np.array([x_min, y_min, bbox_width, bbox_height]).astype(np.int64).tolist()\n",
    "    \n",
    "    if bbox_width>bbox_height:\n",
    "        bbox_pad = (bbox_width-bbox_height)/2\n",
    "        y_min = max((y_min-bbox_pad),0)\n",
    "        bbox_height = min((bbox_height+bbox_pad*2),height-y_min)\n",
    "    else:\n",
    "        bbox_pad = (bbox_height-bbox_width)/2\n",
    "        x_min = max((x_min-bbox_pad),0)\n",
    "        bbox_width = min((bbox_width+bbox_pad*2),width-x_min)\n",
    "    \n",
    "    #margin = max(bbox_width, bbox_height)*margin\n",
    "    #margin = min(max(bbox_width, bbox_height)*margin, 10)\n",
    "    margin = 10\n",
    "    \n",
    "    x_min = max(x_min-margin/2, 0)\n",
    "    y_min = max(y_min-margin/2, 0)\n",
    "    bbox_width = min(bbox_width+margin, width-x_min)\n",
    "    bbox_height = min(bbox_height+margin, height-y_min)\n",
    "    \n",
    "    final_bbox = np.array([x_min, y_min, bbox_width, bbox_height]).astype(np.int64).tolist()\n",
    "    \n",
    "    return origin_bbox, final_bbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bboxtestprocess(size):\n",
    "    nomalize = A.Normalize()\n",
    "\n",
    "    albumentations_transform = A.Compose([\n",
    "        A.LongestMaxSize(size, interpolation = cv2.INTER_AREA),\n",
    "        A.PadIfNeeded(size, size, value=(255,255,255), border_mode=0),\n",
    "        A.Resize(size,size, interpolation = cv2.INTER_AREA),\n",
    "        nomalize,\n",
    "        ToTensorV2()\n",
    "    ]\n",
    "    )\n",
    "    return albumentations_transform\n",
    "\n",
    "class advprop_normal_divide(A.ImageOnlyTransform):\n",
    "    def __init__(self, a=2.0, b=1.0, always_apply=False, p=1):\n",
    "        super().__init__(always_apply=always_apply, p=p)\n",
    "        self.a = a\n",
    "        self.b = b\n",
    "\n",
    "    def apply(self, img, **params):\n",
    "        #return (img.astype(np.float32)/255) * 2.0 - 1.0\n",
    "        return (img.astype(np.float32)/255) * 2.0 - 1.0\n",
    "\n",
    "    def get_transform_init_args_names(self):\n",
    "        return ('a', 'b')\n",
    "\n",
    "\n",
    "class advprop_normal(A.ImageOnlyTransform):\n",
    "    def __init__(self, a=2.0, b=1.0, always_apply=False, p=1):\n",
    "        super().__init__(always_apply=always_apply, p=p)\n",
    "        self.a = a\n",
    "        self.b = b\n",
    "\n",
    "    def apply(self, img, **params):\n",
    "        #return (img.astype(np.float32)/255) * 2.0 - 1.0\n",
    "        return (img.astype(np.float32)) * 2.0 - 1.0\n",
    "\n",
    "    def get_transform_init_args_names(self):\n",
    "        return ('a', 'b')\n",
    "\n",
    "def clsalbval(size, advprop, devide):\n",
    "    if advprop:\n",
    "        if devide:\n",
    "            nomalize = advprop_normal_divide()\n",
    "        else:\n",
    "            nomalize = advprop_normal()\n",
    "    else:\n",
    "        nomalize = A.Normalize()\n",
    "\n",
    "    albumentations_transform = A.Compose([\n",
    "        A.SmallestMaxSize(size, interpolation = cv2.INTER_AREA),\n",
    "        A.CenterCrop(size,size, p=1),\n",
    "        nomalize,\n",
    "        ToTensorV2()\n",
    "    ])\n",
    "    return albumentations_transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "int_to_label = {0:\"종이\", 1:\"종이팩\", 2:\"알류미늄캔\", 3:\"유리\", 4:\"페트\", 5:\"플라스틱\", 6:\"비닐\"}\n",
    "bbox_image_size = 456\n",
    "bbox_margin = 1/8\n",
    "bbox_device = 'cpu'\n",
    "cls_device = 'cpu'\n",
    "cls_image_size = 456\n",
    "cls_advprop = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "bbox_preprocess = bboxtestprocess(bbox_image_size)\n",
    "cls_preprocess = clsalbval(cls_image_size, cls_advprop, devide = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "bbox_model = torch.jit.load('garbage_bbox_jit.pth', masp_location = bbox_device)\n",
    "cls_model = torch.jit.load('garbage_cls_jit.pth', map_location = cls_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "origin_image = cv2.imread('../dataset/garbage_v2/GOPR5444.jpg')\n",
    "origin_image = cv2.cvtColor(origin_image, cv2.COLOR_BGR2RGB)\n",
    "image = bbox_preprocess(image = origin_image)['image']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    bboxes = bbox_model(torch.unsqueeze(image, 0).to(device = bbox_device))[0]\n",
    "    bboxes = np.array([[bboxes[0]*bbox_image_size, bboxes[1]*bbox_image_size, bboxes[2]*bbox_image_size, bboxes[3]*bbox_image_size]]).astype(np.float32).tolist()\n",
    "    multiply = max(origin_image.shape)/bbox_image_size\n",
    "    origin_bboxes = (np.array(bboxes)*multiply).astype(np.int64).tolist()\n",
    "    origin_bbox, new_bboxes = postprocessing(origin_image.shape, origin_bboxes[0][:], bbox_margin)\n",
    "    cuted_image = origin_image[new_bboxes[1]:new_bboxes[1]+new_bboxes[3],new_bboxes[0]:new_bboxes[0]+new_bboxes[2],:]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "cls_input = torch.unsqueeze(cls_preprocess(image = cuted_image)['image'], 0).to(device = cls_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = torch.sigmoid(cls_model(cls_input))\n",
    "rounded_output = torch.round(output)[0].detach().cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "알류미늄캔\n"
     ]
    }
   ],
   "source": [
    "for i,val in enumerate(rounded_output):\n",
    "    if int(val)==1: \n",
    "        print(int_to_label[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'알류미늄캔'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "', '.join([int_to_label[i] for i,val in enumerate(rounded_output) if int(val)==1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Py36 (env_electra)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
